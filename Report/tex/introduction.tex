\section{Introduction}
In this project we aim to create a general purpose tool for sentiment analysis in the software genre called Natural Language Processing. More specifically, this tool should be specialized in predicting sentiment on comments on political articles on social media - which is called Sentiment Analysis\footnote{https://monkeylearn.com/sentiment-analysis/}.

To constitute a general purpose program there are several functionalities that to us must be included. Firstly, the user should have several customizable options for running the program. This could include having direct impact on how the program behaves in specific circumstances or on a more abstract level - change what algorithm is being used. Secondly, the program should - together with a set of easy to understand instructions - be useable for a non-technical individual. Lastly, the program should be able to reliably predict sentiment on comments on all sorts of political articles.

In the project we were to implement two or more different types of algorithms that are able to predict sentiment in sentences. An algorithm that is capable of taking input data and map it into a category is called a classifier.\footnote{https://monkeylearn.com/sentiment-analysis/}
	There are three types of classifiers relevant to this paper: The rule based/lexical classifier, the support vector machine (hereafter SVM) and neural network (hereafter NN). The three different types of classification will be thoroughly described later. The important note here is that two out of the three approaches are using supervised machine learning techniques\footnote{https://www.ibm.com/downloads/cas/GB8ZMQZ3 - page 15}. Explained simply, supervised machine learning uses an arbitrary machine learning algorithm to fit a function to a dataset so the algorithm "learns" the characteristics of the data, and afterwards will be able to predict which class an arbitrary input belongs to. The "supervised" part means that each data entry we feed the algorithm is accompanied by a "label" of what class the entry belongs to. In our case we want the algorithms to be able to tell whether a sentence is positive, neutral or negative. Therefore we need to collect a vast amount of data, so that we can enhance the ability of the algorithm to do qualified guesses. The more and the better data we have, the better results we should get. This also leads to the question: how do we determine if results are good or bad?
	To do this sort of conclusion it is important to collect a large amount of data and tell the algorithm what we believe the right answer is. When the algorithm then makes a qualified guess, we can determine whether that guess was correct according to our labelling. 

How the program is constructed is also an important factor. This is not directly related to the functionality, but it is however very important in relation to how scaleable the program is with an increasingly larger dataset. Therefore we will also go into detail with how the program was constructed, what decisions were made to support the usability of the program and what the result of these changes were.

%TODO: Husk at nævne, at vores accuracies er målt på 100 kørsler med 5 kfold 
