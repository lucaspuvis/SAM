\section{Analysis} \label{analysis}

Shown in table \ref{analysisstat} is a collection of all the results from running our classifiers on the dataset. We also added the result of AFINN and his program in this table. For fairness we ran his program in the exact same way as we ran our rule based classifier, so that the results could easily be compared.

The results of these classifiers have been gathered by training and running the classifier 100 times and taking the mean average of the accuracy results. This does not apply to the baseline-, the random-, AFINN's- and the rule based classifiers, as their results are completely static due to their nature of obtaining sentiment. Furthermore, as they use the complete dataset for obtaining accuracy, their results have been noted with a star. Otherwise, the classifier will have used 90\% (~8100 phrases) of the entire dataset as training data, and 10\% (~900 phrases) as testing data.
\begin{table}[H]
	\begin{tabular}{@{}lll@{}}
		\toprule
		& Accuracy & F1 Score \\ \midrule
		Baseline  & 45.37\%* & 0.2081   \\
		Random    & 33.13\%  & 0.3158   \\
		AFINN     & 52.89\%* & 0.4995   \\
		Lexical   & 56.27\%* & 0.5372   \\
		MML       & 63.01\%  & 0.6345   \\
		SVM       & 67.12\%  & 0.6689   \\
		Multi SVM & 65.86\%  & 0.6501   \\
		NN        & 35.84\%  & 0.3000   \\ \bottomrule
	\end{tabular}
	\caption{Table over results of all classifiers.}
	\label{analysisstat}
\end{table}
It's evident from this table that neither of our baseline classifiers (Baseline and Random) beat any of the other classifiers in accuracy. This proves that the other six classification approaches managed to beat the baseline classifiers.

As can be seen from the table, our lexical analysis did better than what AFINN had produced. This is partly because we have added more words which we've seen used often in our dataset, but most likely it's because we've based our lexicon upon AFINN's. When comparing our program to AFINN's when using AFINN's dataset, his program actually produced slightly higher accuracy, as seen in section \ref{lexical} in table \ref{afinnresult}. Our classifier reached an accuracy of 52.76\% whilst his reached an accuracy of 52.89\%, which is in total an increase of .13\% points. It's also possible to enable the support for emoticons when using AFINN to classify. Since we also have some emojis in our lexicon for our rule based classifier, it would only be fair to test his emoticon support too. Enabling this setting increased the accuracy by .04\% points, which means that our lexical analysis approach still performs better.

An interesting result amongst these is the fact that the LSTM neural network did not manage to beat either of the naive approaches to sentiment analysis. Both AFINN and the lexical analysis managed to beat its accuracy, as well as our baseline classifier. As mentioned in section \ref{training}, we did have some difficulties getting the neural network to provide good and consistent results. The fact that it doesn't manage to beat either of the naive approaches does indicate that there are some major issues with how we handled the neural network, although we do not have a definitive reason to explain the poor results.

On the opposite end, all the SVM's did better than all the naive approaches and our baseline classifiers. The best performing of our SVMs was the regular SVM. Thereafter is the Multi SVM, and at last place is the MML SVM. MML SVM being in last place is no surprise to anyone. As we could not configure it further than the very basics, it was very hard to improve the accuracy of this SVM. The Multi SVM, although a nice idea, was not as good as our regular SVM in practice. Comparing these machine learning approaches to the neural network reveals a large difference. The neural network pales in comparison to all the SVMs. 

All in all it's clear to see that our SVM reigns supreme when it comes to accuracy. Trailing slowly behind it is the Multi SVM, followed by the MML SVM.

%TODO: Transition?

