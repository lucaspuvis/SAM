\subsection{Krippendorff’s alpha}
Krippendorff’s alpha is a statistical method for calculating the overall agreement between a number of annotators, independently annotating a dataset. 
We used an online tool named “ReCal” \cite{ReCal} to calculate our alpha values.
Since every member of the group independently annotated the words in the lexicon used for the rule-based classifier, the alpha value can be used to prove the reliability of the lexicon.
We reached an alpha value of 0.922 on our annotation of our lexicon. This indicates that we had a high level of agreement, and that the ratings are correct.
In the beginning of the project, we had the assumption that we would have a high level of agreement. Luckily, we decided to all annotate the first 100 sentences and compare the ratings.
When calculating Krippendorff’s alpha for the initial annotations, we reached an agreement of 0.338. After realizing this, we decided to look at the initial annotations, discuss the ratings and build a set of rules for the further annotation.
This turned out to be a great idea, as later in the project when we checked the dataset for inconsistencies and disagreeable ratings, we only had to discuss a small amount of sentences.

