\subsection{Classification using a Neural Network}
We implemented our neural network using the Keras\cite{Keras} library. 
This allowed us to build and train a neural network quickly while not losing functionality.

One of the most important parts of our neural network is the LSTM (Long Short Term Memory) layer. 
This is where the tokens in the sentences are put in relation to one another in order to be able to predict the sentiment of the sentences.
LSTM is a type of recurrent neural network\footnote{A type of neural network with connections to previous states. This allows the network to work on multiple parts of the same input data}, but with the added feature that it learns which information to keep and what to forget.
This essentially allows the network to look at multiple parts of the input at once. 
This makes LSTM networks very powerful when applied to domains like speech recognition, handwriting detection and natural language processing.

\subsubsection{Preprocessing}
Before a sentence is ready to be sent through the neural network, we need to do some preprocessing. This consists of the sentence being tokenized (converted to a sequence of tokens) and normalization of the token sequences. These steps are described in the following sections:

\subsubsection{Data loading}
Our dataset is loaded into the program the same way as everywhere else. However, since our neural network outputs predictions between 0 and 1, we need to make sure our labels are within this range.
We do this by changing our negative labels to 0, our neutral labels to 0.5 and our positive labels to 1.

\subsubsection{Tokenization}
Since our neural network cannot read strings of text we need to find another way to represent our data. We do this by converting our sentences into sequences of integers using a tokenizer. This reads the sentence and maps every word to a value if the tokenizer has seen the word when we train the neural network. If it finds a word it has not seen before, a token corresponding to the word “UNKNOWN” is added instead. The tokenizer also removes words that are outside the 10000 most used words in our dataset. This ensures that we ignore words that occur so rarely that they cannot be used to predict sentiment, and also reduces the training time as the number of words the network has to learn is reduced.
The tokenizer used by our classifier contains mappings for the 10000 most used words in our dataset.

\subsubsection{Sentence normalization}
After the sentences have been tokenized, we need to normalize their lengths. 
We decided to use the length of the longest sentence in the dataset, since we wanted to avoid ignoring potentially important data.
Normalizing the length of the sentences allows us to train the network using multiple sentences per iteration, rather than one at a time. This greatly reduces the training time of the network.

\subsubsection{Network structure}
Our network consists of the following three main layers. The embedding layer, which accepts our preprocessed sentences and converts them into feature vectors for later use in the network, an LSTM layer which does the predictions, and a dense layer, which completes the prediction, and returns the predicted sentiment of the sentence.

The network outputs a floating point value between 0 and 1, where we assume 1 is positive, 0 is negative and 0.5 is neutral.
Since the rest of our program uses the labels -1, 0 and 1, we need to convert the predictions to the correct labels. We do this by converting all predictions below 0.33 to -1, all above 0.66 to 1 and predictions between 0.33 and 0.66 to 0.

\subsubsection{Training} \label{training}
We train our neural network using the labelled sentences in our dataset. After the sentences have been preprocessed, they are ready to be used for training the network.
The network is trained over a number of iterations called epochs. For every epoch, the training data is split into smaller sets called batches. These batches are then sent through the network, which attempts to learn from data within them. When all the batches have been through the network, a new epoch begins and the batches are sent through again.
After every epoch, a set of validation data is sent through the network and the accuracy of the predictions are evaluated. In order to prevent the network from overfitting on our training data, we allowed the network to stop training if there were no improvement in accuracy for the previous 5 epochs when predicting on the validation data. Additionally, to ensure the final network is as accurate as possible without overfitting, we only save the model with the best accuracy on our testing dataset.

When training the neural network, we reach an accuracy around 55\% on our testing dataset.
However, when using the network at any other time, we only hit an accuracy of 33\%. We have not been able to find the exact reason, but 
we believe the reason for the low accuracy is most likely due to us finding it difficult to configure the neural network. 
